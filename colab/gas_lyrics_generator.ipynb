{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILxfzLKBA5xI"
   },
   "source": [
    "Сделано на основе https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aHK2CYygXom"
   },
   "source": [
    "## Import TensorFlow and related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2LmLTREBf5ng"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Other imports for processing data\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "397LUEtGzazw"
   },
   "source": [
    "Может обучить сначала на куче текстов, а потом трансформнуть для сектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxDIPSDpzkD6",
    "outputId": "fb3e7b01-0cba-4dcf-f72e-019923ff31a2"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/d0rj/RusLit/archive/refs/heads/main.zip --output-document ./literature.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuELN1cj0EbJ"
   },
   "outputs": [],
   "source": [
    "!unzip literature.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aaIPlQXJ6iyG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "raw_literature = []\n",
    "for (root,dirs,files) in os.walk('./RusLit', topdown=True):\n",
    "  for f in files:\n",
    "    f_name = os.path.join(root, f)\n",
    "    if f_name.endswith('.txt'):\n",
    "      x = 100\n",
    "      #print(f_name)\n",
    "      with open(f_name, 'r', encoding='utf-8') as file:\n",
    "        try:\n",
    "          paragraph = file.read()\n",
    "          raw_literature.extend([paragraph[i: i + x] for i in range(0, len(paragraph), x)])\n",
    "        except:\n",
    "          pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1SZ2khf9j8Z",
    "outputId": "363d49ae-bdfc-477b-972f-4a0d417f097f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392337\n",
      "['Александр Блок  endline\\n endline\\nВОЗМЕЗДИЕ  endline\\n endline\\n                                                   Юность - это возмезд', 'ие. endline\\n                                                                 Ибсен  endline\\nПРЕДИСЛОВИЕ   endline\\n      endline\\n    ', ' Не  чувствуя ни нужды, ни охоты заканчивать поэму, полную революционных endline\\nпредчувствий,  в  года,  ко', 'гда  революция  уже произошла, я хочу предпослать endline\\nнаброску  последней  главы  рассказ  о  том, как п', 'оэма родилась, каковы были endline\\nпричины ее возникновения, откуда произошли ее ритмы.  endline\\n     Интересно  и  ', 'небесполезно  и для себя, и для других припомнить историю endline\\nсобственного  произведения. К тому же нам,', ' счастливейшим или несчастливейшим endline\\nдетям  своего  века,  приходится помнить всю свою жизнь; все годы', ' наши резко endline\\nокрашены  для  нас,  и  -  увы!  -  забыть их нельзя, - они окрашены слишком endline\\nнеизгладимо', ',  так  что каждая цифра кажется написанной кровью; мы и не можем endline\\nзабыть этих цифр; они написаны на ', 'наших собственных лицах.  endline\\n      endline\\n     Поэма  \"Возмездие\"  была  задумана  в 1910 году и в главных чер']\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_literature))\n",
    "raw_literature_2 = list(map(lambda x: x.replace('\\n\\n', ' \\n'), raw_literature))\n",
    "#print(songs[0])\n",
    "raw_literature_2 = list(map(lambda x: x.replace('\\n', ' endline\\n'), raw_literature_2))\n",
    "#print(songs[0])\n",
    "print(raw_literature_2[:10])\n",
    "dataset = pd.DataFrame(raw_literature_2[:10_000], columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIWhZ4uwAjLr"
   },
   "source": [
    "Нет! Мы нафигачим сюда текстов Сектора газа!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "unzJsmM4Aoke",
    "outputId": "2f330ad5-8be9-4055-a09d-84c0d1504f5a"
   },
   "outputs": [],
   "source": [
    "from get_sector_songs import get_sector_songs\n",
    "dataset_2 = get_sector_songs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWbMN_19jfRT"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRmPPJegovBe",
    "outputId": "429e6a69-ca61-4c7f-a4aa-f86de85a519e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "(96369, 13)\n",
      "(2043, 13)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "  # Fit a Tokenizer on the corpus\n",
    "  if num_words > -1:\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "  else:\n",
    "    tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "  # Remove all other punctuation\n",
    "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "  # Make it lowercase\n",
    "  dataset[field] = dataset[field].str.lower()\n",
    "  # Make it one long string to split by line\n",
    "  lyrics = dataset[field].str.cat()\n",
    "  corpus = lyrics.split('\\n')\n",
    "  # Remove any trailing whitespace\n",
    "  for l in range(len(corpus)):\n",
    "    corpus[l] = corpus[l].rstrip()\n",
    "  # Remove any empty lines\n",
    "  corpus = [l for l in corpus if l != '']\n",
    "\n",
    "  return corpus\n",
    "\n",
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "  # Fit a Tokenizer on the corpus\n",
    "  if num_words > -1:\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "  else:\n",
    "    tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  return tokenizer\n",
    "\n",
    "# Read the dataset from csv - this time with 250 songs\n",
    "#dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:250]\n",
    "# Create the corpus using the 'text' column containing lyrics\n",
    "def create_imput_data(corpus, max_sequence_len=None):\n",
    "  sequences = []\n",
    "  for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "      n_gram_sequence = token_list[:i+1]\n",
    "      sequences.append(n_gram_sequence)\n",
    "\n",
    "  # Pad sequences for equal input length \n",
    "  if max_sequence_len is None:\n",
    "    max_sequence_len = max([len(seq) for seq in sequences])\n",
    "  sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "  # Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "  input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "  # One-hot encode the labels\n",
    "  one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "  return max_sequence_len, input_sequences, one_hot_labels\n",
    "\n",
    "corpus = create_lyrics_corpus(dataset, 'text')\n",
    "# Tokenize the corpus\n",
    "tokenizer = tokenize_corpus(corpus, num_words=6000)\n",
    "total_words = tokenizer.num_words\n",
    "\n",
    "# There should be a lot more words now\n",
    "print(total_words)\n",
    "\n",
    "max_sequence_len, input_sequences, one_hot_labels = create_imput_data(corpus)\n",
    "_, input_sequences_2, one_hot_labels_2 = create_imput_data(create_lyrics_corpus(dataset_2, 'text'), max_sequence_len)\n",
    "print(input_sequences.shape)\n",
    "print(input_sequences_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cECbqT-blMk-"
   },
   "source": [
    "### Train a (Better) Text Generation Model\n",
    "\n",
    "Сначала Корпус русской классики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nHOp6uWlP_P",
    "outputId": "d0884ff6-594b-4c4e-8cb6-bc915af1bd9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3012/3012 [==============================] - 72s 22ms/step - loss: 5.8602 - accuracy: 0.2916\n",
      "Epoch 2/5\n",
      "3012/3012 [==============================] - 69s 23ms/step - loss: 5.4459 - accuracy: 0.2943\n",
      "Epoch 3/5\n",
      "2221/3012 [=====================>........] - ETA: 25s - loss: 5.2319 - accuracy: 0.2976"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, AdditiveAttention\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 128, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdmK85snLUdb"
   },
   "source": [
    "## Трансфер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13aNar5bLani",
    "outputId": "9c1da794-9d0d-4c33-e792-29118f855eb3"
   },
   "outputs": [],
   "source": [
    "model.save('fitsrt_step_model') # saves compiled state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwgytLtuMqlt",
    "outputId": "a6c7a094-35ed-4b42-ab10-5f6b838277eb"
   },
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model('fitsrt_step_model')\n",
    "history2 = model2.fit(input_sequences_2, one_hot_labels_2, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgvIz20nlQcq"
   },
   "source": [
    "### View the Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rOqmmarvlSLh",
    "outputId": "ac83ca17-4281-48a1-b812-e6edf4cbb81d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upgJKV8_oRU9"
   },
   "source": [
    "### Varying the Possible Outputs\n",
    "\n",
    "In running the above, you may notice that the same seed text will generate similar outputs. This is because the code is currently always choosing the top predicted class as the next word. What if you wanted more variance in the output? \n",
    "\n",
    "Switching from `model.predict_classes` to `model.predict_proba` will get us all of the class probabilities. We can combine this with `np.random.choice` to select a given predicted output based on a probability, thereby giving a bit more randomness to our outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee7WKgRGrJy1",
    "outputId": "e9af5ec8-7ffc-4a12-b022-f02347e71e51"
   },
   "outputs": [],
   "source": [
    "# Use this process for the full output generation\n",
    "seed_text = \"Грустный клоун\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "  predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\n",
    "                               p=predicted_probs)\n",
    "  output_word = \"\"\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index == predicted:\n",
    "      output_word = word\n",
    "      break\n",
    "  seed_text += \" \" + output_word\n",
    "seed_text = seed_text.replace('endline', '\\n')\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlHj-HArUNuf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEEkXg6pUOVl",
    "outputId": "55922831-85f6-4248-cf7c-d24d16ff9b07"
   },
   "outputs": [],
   "source": [
    "# Use this process for the full output generation\n",
    "seed_text = \"Грустный клоун\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "  predicted_probs = model2.predict(token_list, verbose=0)[0]\n",
    "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\n",
    "                               p=predicted_probs)\n",
    "  output_word = \"\"\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index == predicted:\n",
    "      output_word = word\n",
    "      break\n",
    "  seed_text += \" \" + output_word\n",
    "seed_text = seed_text.replace('endline', '\\n')\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "l10c04_nlp_optimizing_the_text_generation_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
