{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakreved/ML_Playground/blob/main/colab/gas_lyrics_generator.ipnb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделано на основе https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb"
      ],
      "metadata": {
        "id": "ILxfzLKBA5xI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Optimizing the Text Generation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCxhW3mtLmfb"
      },
      "source": [
        "You've already done some amazing work with generating new songs, but so far we've seen some issues with repetition and a fair amount of incoherence. By using more data and further tweaking the model, you'll be able to get improved results. We'll once again use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics) here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bf5FVHfganK"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нет! Мы нафигачим сюда текстов Сектора газа!"
      ],
      "metadata": {
        "id": "sIWhZ4uwAjLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "song_1 = '''С матершиной мы родились,\n",
        "C матершиной мы живём,\n",
        "С матершиной мы учились,\n",
        "C матершиной и помрём,\n",
        "Матершину мы вкушали\n",
        "C материнским молоком,\n",
        "С матершиной мой папаша\n",
        "Бил мамашу кулаком.\n",
        "\n",
        "Вы не ждите, что в припеве\n",
        "Заругаюсь матом я,\n",
        "Я б, конечно, заругался,\n",
        "Только очередь твоя!\n",
        "И подкалывать не надо,\n",
        "Мне на это наплевать,\n",
        "Матершинные слова\n",
        "Не буду я употреблять!\n",
        "\n",
        "Матом кроют генералы,\n",
        "Матом кроют продавцы,\n",
        "Матом кроют прокуроры\n",
        "И родные отцы.\n",
        "Все, как будто бы святые,\n",
        "А приди к любому в дом,\n",
        "Через каждые два слова\n",
        "Каждый ложит матюгом.\n",
        "\n",
        "Вы не ждите, что в припеве\n",
        "Заругаюсь матом я,\n",
        "Я б, конечно, заругался,\n",
        "Только очередь твоя!\n",
        "И подкалывать не надо,\n",
        "Мне на это наплевать,\n",
        "Матершинные слова\n",
        "Не буду я употреблять!\n",
        "Источник teksty-pesenok.ru\n",
        "\n",
        "Без русcкого мата\n",
        "Не прожить нам и дня,\n",
        "Отведи мою душу,\n",
        "Обложи ты меня,\n",
        "Матершиной нашей русcкой\n",
        "Обложи и не с*ы,\n",
        "Как учили нас деды,\n",
        "Как учили отцы.\n",
        "\n",
        "Вы не ждите, что в припеве\n",
        "Заругаюсь матом я,\n",
        "Я б, конечно, заругался,\n",
        "Только очередь твоя!\n",
        "И подкалывать не надо,\n",
        "Мне на это наплевать,\n",
        "Матершинные слова\n",
        "Не буду я употреблять'''\n",
        "\n",
        "song_2 = '''Был хорошо, было так легко,\n",
        "Но на шею бросили аркан,\n",
        "Солнечный огонь,\n",
        "Атмосферы бронь,\n",
        "Пробевал, но не пробил туман\n",
        "\n",
        "И мертвый месяц еле освещает путь,\n",
        "И звезды давят нам на грудь - не продохнуть,\n",
        "И воздух ядовит как ртуть,\n",
        "Нельзя свернуть, нельзя шагнуть,\n",
        "И не пройти нам этот путь в такой туман\n",
        "\n",
        "А куда шагнуть?\n",
        "Бог покажет путь,\n",
        "Бог для нас всегда бесплотный вождь,\n",
        "Нас бросает в дрожь, вдруг начался дождь,\n",
        "Нас добьет конкретный сильный дождь\n",
        "\n",
        "И месяц провоцирует нас на обман,\n",
        "И испарения земли бьет как дурман,\n",
        "И каждый пень нам как капкан,\n",
        "И хлещет кровь из наших ран,\n",
        "И не пройти нам этот путь в такй туман\n",
        "\n",
        "Все пошло на сдвиг, наша жизнь как миг,\n",
        "Коротка как юбка у путан,\n",
        "Нам все непочем через левое плечо, плюнем и пойдем через туман\n",
        "Источник teksty-pesenok.ru\n",
        "\n",
        "Пусть мертвый месяц еле освещает путь,\n",
        "\n",
        "Пусть звезды давят нам на грудь - не продохнуть,\n",
        "Пусть воздух ядовит как ртуть и пусть не видно где свернуть\n",
        "Но мы пройдем опасный путь через туман,\n",
        "\n",
        "Пусть месяц провоцирует нас на обман,\n",
        "Пусть испарение земли бьет как дурман,\n",
        "Пусть каждый пень нам как капкан ,\n",
        "Пусть хлещет кровь из наших ран,\n",
        "Но мы пройдем с тобою путь через туман\n",
        "Но мы пройдем с тобою путь через туман\n",
        "но мы пройдем опасный путь через туман'''\n",
        "\n",
        "song_3 = '''\n",
        "Я копаюсь на помойке, как червяк.\n",
        "С детства жизнь моя наперекосяк.\n",
        "Канализационный люк - моя дверь,\n",
        "Но я счастлив по-своему, поверь.\n",
        "Двадцать лет назад сгорел родной мой дом.\n",
        "Документы, деньги - все сгорело в нем.\n",
        "И теперь я побираюсь двадцать лет -\n",
        "Кому нужен старый, никудышный дед.\n",
        "\n",
        "А я бычок подниму, горький дым затяну,\n",
        "Покурю и полезу домо-ой.\n",
        "Не жалейте меня, я прекрасно живу,\n",
        "Только кушать охота порой.\n",
        "А я бычок подниму, горький дым затяну,\n",
        "Люк открою, полезу домо-ой.\n",
        "Не жалейте меня, я прекрасно живу,\n",
        "Только кушать охота порой.\n",
        "\n",
        "Эй, ребята, как допьете вы вино,\n",
        "Мне бутылки вы оставьте за одно.\n",
        "Пожалейте вы несчастного БОМЖу.\n",
        "Я их в сумку аккуратно положу.\n",
        "Отнесу я завтра их в приемный пункт.\n",
        "Мне за них 60 копеек отдадут.\n",
        "Я куплю буханку хлеба и сырок,\n",
        "Чтобы с голоду не протянуть мне ног.\n",
        "\n",
        "А я бычок подниму, горький дым затяну,\n",
        "Покурю и полезу домо-ой.\n",
        "Не жалейте меня, я прекрасно живу,\n",
        "Только кушать охота порой.\n",
        "А я бычок подниму, горький дым затяну,\n",
        "Люк открою, полезу домо-ой.\n",
        "Источник teksty-pesenok.ru\n",
        "Не жалейте меня, я прекрасно живу,\n",
        "Только кушать охота порой.\n",
        "\n",
        "Ах, подайте, люди, мне, ради Христа.\n",
        "Не мила мне жизнь, я от нее устал.\n",
        "Помоги, прохожий милый, пятаком,\n",
        "Сжалься надо мной, над бедным стариком.\n",
        "Ах, отпусти меня, товарищ старшина.\n",
        "Я простой советский БОМЖ, а не шпана.\n",
        "Я не сделал ведь плохого никому.\n",
        "Так за что меня берете, не пойму?\n",
        "\n",
        "А я бычок подниму, горький дым затяну,\n",
        "Покурю и полезу домо-ой.\n",
        "Не жалейте меня, я прекрасно живу,\n",
        "Только кушать охота порой.\n",
        "А я бычок подниму, горький дым затяну,\n",
        "Люк открою, полезу домо-ой.\n",
        "Не жалейте меня, я прекрасно живу,\n",
        "Только кушать охота порой.'''\n",
        "\n",
        "song_4 = '''Бери свою метлу, полетели на шабаш,\n",
        "Возьми с собой апокалипсис - символ наш:\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Сегодня каждой твари будет не в мочь,\n",
        "Сегодня слуги дьявола хоронят свою дочь.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Преданы анафеме с тобою мы давно,\n",
        "Наше имя в черный список занесено.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Мы с ведьмами в ромашку поиграем в зту ночь,\n",
        "Мы изнас*луем похороненную дочь.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Источник teksty-pesenok.ru\n",
        "И пусть на нас не сердится папаша Люцифер,\n",
        "Мы перед ним чисты, без всяких афер.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Ведь утром, на рассвете, мы подожмем хвосты,\n",
        "А в полдень инквизиторы нас кинут на костры.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Сегодня настает вальпургиева ночь.\n",
        "Сегодня настает вальпургиева ночь.'''\n",
        "\n",
        "song_5 = '''Солнце осветило горизонт\n",
        "Утро оборвало мой сладкий сон\n",
        "Я проснулся я был поражён\n",
        "Ощутил годам урон\n",
        "Словно в первый раз я увидел свет\n",
        "Словно в первый раз я был им согрет\n",
        "Годы ощутил хоть я и не дед\n",
        "Мне сегодня тридцать лет\n",
        "\n",
        "В этот день родили меня на свет\n",
        "В этот день с иголочки я одет\n",
        "В этот день теплом вашим я согрет\n",
        "Мне сегодня тридцать лет\n",
        "В этот день скажу юности привет\n",
        "В этот день я в зрелость возьму билет\n",
        "В этот день и водка не во вред\n",
        "Мне сегодня тридцать лет\n",
        "\n",
        "Солнце наливаясь вошло в зенит\n",
        "Музыка в моих ушах звенит\n",
        "Не грусти и даже не делай вид\n",
        "Каждая нота мне говорит\n",
        "Я рукой нащупал свой карман\n",
        "Он мне намекнул что я буду пьян\n",
        "Говорит мне день держись братан\n",
        "Лучше протирай стакан\n",
        "\n",
        "В этот день родили меня на свет\n",
        "В этот день с иголочки я одет\n",
        "В этот день теплом вашим я согрет\n",
        "Мне сегодня 30 лет\n",
        "В этот день скажу юности привет\n",
        "В этот день я в зрелость возьму билет\n",
        "В этот день и водка не во вред\n",
        "Мне сегодня 30 лет\n",
        "\n",
        "Источник teksty-pesenok.ru\n",
        "Солнце опустилось за горизонт\n",
        "Дома у меня стаканов звон\n",
        "Собрались друзья со всех сторон\n",
        "Друг врубай магнитофон\n",
        "Я имею право сегодня пить\n",
        "Я имею право сегодня жить\n",
        "Я имею право про всё забыть\n",
        "Да чего там говорить\n",
        "\n",
        "В этот день родили меня на свет\n",
        "В этот день с иголочки я одет\n",
        "В этот день теплом вашим я согрет\n",
        "Мне сегодня тридцать лет\n",
        "В этот день скажу юности привет\n",
        "В этот день я в зрелость возьму билет\n",
        "В этот день и водка не во вред\n",
        "Мне сегодня тридцать лет\n",
        "Мне сегодня тридцать лет'''\n",
        "\n",
        "song_6 = '''Эй, подруга, выходи скорей во двор.\n",
        "Я специально для тебя гитару припер.\n",
        "Я сыграю для тебя на аккордах на блатных.\n",
        "Ну а коль не выйдешь ты, то получишь под дых.\n",
        "Возле дома твоего-о, возле дома твоего.\n",
        "Возле дома твоего-а, возле дома твоего.\n",
        "\n",
        "Эй, подруга, выходи-ка на крыльцо.\n",
        "И не слушай ругань матерную матери с отцом.\n",
        "Я тебя аккуратно к сеновалу отнесу.\n",
        "Ну а коль не выйдешь ты, все заборы обоссу.\n",
        "\n",
        "Возле дома твоего-о, возле дома твоего.\n",
        "Возле дома твоего-а, возле дома твоего.\n",
        "Источник teksty-pesenok.ru\n",
        "\n",
        "Эй, залетка, выходи послушать рок.\n",
        "Я специально для тебя магнитолу приволок.\n",
        "Ну а коль не выйдешь ты, то учти, ядрена медь:\n",
        "На весь хутор под гитару песни матом буду пе-е-е-ть.\n",
        "\n",
        "Возле дома твоего-о, возле дома твоего.\n",
        "Возле дома твоего-а, возле дома твоего-а-а-а-а-а'''\n",
        "\n",
        "song_7 = '''Скорый поезд к дому мчится.\n",
        "Полечу домой как птица,\n",
        "Полечу как птица я!\n",
        "Жизнь начнётся без авралов,\n",
        "Сундуков и генералов\n",
        "Демобилизация! - 2х\n",
        "Я приеду к тебе вскоре,\n",
        "Будет спать спокойно город\n",
        "В клёнах и акациях.\n",
        "Я прижмусь к тебе щекою,\n",
        "Ты смахнешь слезу рукою.\n",
        "Демобилизация!\n",
        "И пройдёмся под луною,\n",
        "Рядом с будущей женою,\n",
        "Каблучками цокая!\n",
        "Не боясь за то в награду\n",
        "Получить пяток к наряду.\n",
        "Демобилизация! - 2х\n",
        "\n",
        "И поднимем мы бокалы\n",
        "Источник teksty-pesenok.ru\n",
        "В самых лучших ресторанах,\n",
        "Не боясь нотации.\n",
        "Не боясь за эту шутку\n",
        "Получить 15 суток.\n",
        "Демобилизация! - 2х\n",
        "Скорый поезд к дому мчится.\n",
        "Прилетел домой как птица,\n",
        "Прилетел как птица я!\n",
        "Снял погоны и петлицы,\n",
        "И уже успел напиться\n",
        "Демобилизация!'''\n",
        "\n",
        "song_8 = '''Сигарета мелькает во тьме\n",
        "Ветер пепел в лицо швырнул мне\n",
        "И обугленный фильтр\n",
        "На пальцах мне оставил ожог\n",
        "Скрипнув сталью открылася дверь\n",
        "Ты идёшь ты моя теперь\n",
        "Я приятную дрожь ощущаю с головы до ног\n",
        "\n",
        "Ты со мною забудь обо всём\n",
        "Эта ночь нам покажется сном\n",
        "Я возьму тебя и прижму как родную дочь\n",
        "Нас окутает дым сигарет\n",
        "Ты уйдёшь как настанет рассвет\n",
        "И следы на постели напомнят про счастливую ночь\n",
        "\n",
        "Эротичный лунный свет\n",
        "Запретит сказать тебе нет\n",
        "И опустится плавно на пол всё твоё б*льё\n",
        "Шум деревьев и ветер ночной\n",
        "Стон заглушат твой и мой\n",
        "И биение сердца пылающего адским огнём\n",
        "\n",
        "Ты со мною забудь обо всём\n",
        "Эта ночь нам покажется сном\n",
        "Я возьму тебя и прижму как родную дочь\n",
        "Нас окутает дым сигарет\n",
        "Ты уйдёшь как настанет рассвет\n",
        "И следы на постели напомнят про счастливую ночь\n",
        "Источник teksty-pesenok.ru\n",
        "\n",
        "Твои бёдра в сиянии Луны\n",
        "Так прекрасны и мне так нужны\n",
        "Кровь тяжёлым напором ударит прямо в сердце мне\n",
        "Груди плавно качнутся в ночи\n",
        "Слышишь как моё сердце стучит\n",
        "Два пылающих тела сольются в ночной тишине\n",
        "\n",
        "Ты со мною забудь обо всём\n",
        "Эта ночь нам покажется сном\n",
        "Я возьму тебя и прижму как родную дочь\n",
        "Нас окутает дым сигарет\n",
        "Ты уйдёшь как настанет рассвет\n",
        "И следы на постели напомнят про счастливую ночь\n",
        "И следы на постели напомнят про счастливую ночь'''\n",
        "\n",
        "song_9 = '''Я друзья решил женится правда вот те крест\n",
        "У меня к невесте рос всё больший интерес\n",
        "Анадысь привёл её к родителям своим\n",
        "Было сразу видно что она по нраву им\n",
        "И она меня припёрла к мамочке своей\n",
        "Та поставила пузырь на зятёк мой пей\n",
        "Напякла бляны со с маслом наварила щей\n",
        "И я понял что она клёвая ваще\n",
        "\n",
        "Тёща моя ласковая\n",
        "Теща моя заботливая\n",
        "Молодая озорная ты родная мать\n",
        "Чего больше детям пожелать\n",
        "\n",
        "Свадьба пролетела как фанера над Москвой\n",
        "Стала тёща как собака очень очень злой\n",
        "Я забыл вкус самогона позабыл бляны\n",
        "Тёща говорит они мне на хр*н не нужны\n",
        "\n",
        "К тёще я со всей душою мам налей 100 грамм\n",
        "А она мне дулю в харю ни фига не дам\n",
        "Я такую тёщу не желаю и врагам\n",
        "Похудел я за 2 дня на 20 килограмм\n",
        "\n",
        "Тёща моя злая свинья\n",
        "Тёща моя коростовая\n",
        "Харя злая вот такая хочется плевать\n",
        "На фига нужна вторая мать\n",
        "\n",
        "Я не выдержу возьму и ножик наточу\n",
        "Тёщу я свою в натуре скоро замочу\n",
        "Полечу поколочу по роже постучу\n",
        "В общем я вторую мать малость поучу\n",
        "\n",
        "Эту песню сочинял я ровно 10 лет\n",
        "Мне надеюсь все зятья передадут привет\n",
        "Тёща кровь сосёт из нас прямо как вампир\n",
        "Я надеюсь что мне хором подпоёт весь мир\n",
        "\n",
        "Тёща моя злая свинья\n",
        "Тёща моя коростовая\n",
        "Харя злая вот такая хочется плевать\n",
        "На фига нужна вторая мать'''\n",
        "\n",
        "song_10 = '''Мне на днях исполнилось 16 лет,\n",
        "Захотел я двухколёсный драндулет,\n",
        "Я к батяне подошёл, сказал: \"Купи,\n",
        "Ты на \"Яву\" мне кусочек накопи!\"\n",
        "\n",
        "Мой папаша был хронический алкаш,\n",
        "Но на счастье на него напала блаж,\n",
        "Он в квартире все бутылки пособрал,\n",
        "И на пару тысяч он посуды сдал,\n",
        "\n",
        "\"Яву\", \"Яву\", взял я на халяву!\n",
        "\"Яву\", \"Яву\", взял я на халяву!\n",
        "\n",
        "Я пинком заехал в заводной рычаг,\n",
        "Дай Бог, чтоб движок мой мощный не зачах,\n",
        "До отказа ручку газа накрутил ,\n",
        "Тьфу ты чёрт, рукой я бабку зацепил!\n",
        "\n",
        "Ну и пусть, была бы техника цела,\n",
        "Бабок много \"Ява\", у меня одна,\n",
        "Бабку люди обнаружат по утру,\n",
        "А я тачку нежно тряпочкой протру.\n",
        "\n",
        "\"Ява\", \"Ява\"!Бабка, ух, разява!\n",
        "\"Ява\", \"Ява\"!Бабка, ух, разява!\n",
        "\n",
        "Подвезти раз попросил меня дружок,\n",
        "Дал я гари так, что покраснел движок,\n",
        "Вроде тихо он сидел и не дрожал,\n",
        "А сзади всё сиденье, с*ка обдристал!\n",
        "\n",
        "Раз подругу посадил на мотоцикл,\n",
        "У неё от страха прекратился цикл,\n",
        "Источник teksty-pesenok.ru\n",
        "Платье ветром дуло обнажился срам,\n",
        "И затычки разлетелись по кустам.\n",
        "\n",
        "\"Ява\", \"Ява\", не дрожи ш*лава!\n",
        "\"Ява\", \"Ява\", не дрожи ш*лава!\n",
        "\n",
        "Как-то пьяный возвращался я домой,\n",
        "Гнал на всю катушку мотоцикл свой,\n",
        "Было весело, я громко песни пел,\n",
        "И на пень трухлявый спьяну налетел,\n",
        "В тривелюшки всё погнулось колесо,\n",
        "Метров пять я носом пропахал песок,\n",
        "Я к утру домой обломки принесу,\n",
        "Был я с \"Явой\", а теперь я отс*су.\n",
        "\n",
        "\"Ява\", \"Ява\", пососи раззява!\n",
        "\"Ява\", \"Ява\", пососи раззява!'''\n",
        "\n",
        "song_10 = '''Из-за леса выезжает\n",
        "Конная милиция.\n",
        "Становись-ка девки раком —\n",
        "Будет репетиция!\n",
        "\n",
        "Я приехала в колхоз\n",
        "Имени Мичурина.\n",
        "Так и знала отъе.ут\n",
        "Словно сердце чуяло!\n",
        "\n",
        "Мимо тёщиного дома\n",
        "Я без шуток не хожу.\n",
        "То им х*р в забор просуну,\n",
        "То им ж*пу покажу!\n",
        "\n",
        "Я на Севере была\n",
        "Золото копала.\n",
        "Если б не моя п*зда,\n",
        "С голоду пропала!\n",
        "\n",
        "О-па, о-па, зелёная ограда,\n",
        "Девки вые.ли п*па — так ему и надо!\n",
        "\n",
        "Hа мосту стоял прохожий\n",
        "Hа е.ёну мать похожий.\n",
        "Вдруг откуда ни возьмись\n",
        "Появился в рот е.ись!\n",
        "\n",
        "Полюбила парня я,\n",
        "Да оказался без х*я.\n",
        "Да на х*я ж мне без х*я,\n",
        "Когда с х.ем до х*я!\n",
        "\n",
        "Как у нас на мокушке\n",
        "Соловей е*ет кукушку.\n",
        "Только слышно на суку:\n",
        "\"Чирик, п*здык, х*як, ку-ку!\"\n",
        "\n",
        "Мы с милёнком у метра\n",
        "Целовались до утра.\n",
        "Целовались бы ещё,\n",
        "Да болит влагалищо!\n",
        "\n",
        "О-па, о-па, зелёная ограда,\n",
        "Девки вые.ли п*па — так ему и надо!\n",
        "\n",
        "А девки в озере купались,\n",
        "Х*й резиновый нашли.\n",
        "Целый день они е*ались,\n",
        "Даже в школу не пошли!\n",
        "\n",
        "Мой милёнок под забором\n",
        "Х*й берёзовый нашёл.\n",
        "Примеряли всем народом —\n",
        "Hикому не подошёл!\n",
        "\n",
        "А из-за леса из-за гор\n",
        "Показал мужик топор.\n",
        "Hо не просто показал:\n",
        "Его к х.ю привязал!\n",
        "\n",
        "Мы с милёнком целовались,\n",
        "Стоя у завалинки.\n",
        "А я стояла и с.ала\n",
        "Ему на белы валенки!\n",
        "\n",
        "О-па, о-па, срослась п*зда и ж*па!\n",
        "Этого не может быть — промежуток должен быть!\n",
        "\n",
        "А Маньке целочку порвали,\n",
        "Саньку в ж.пу вые.ли.\n",
        "Опосля в рояль наср*ли —\n",
        "Чудно время провели!\n",
        "\n",
        "Мы с милёночком е.лись\n",
        "В сорокоградусный мороз.\n",
        "Ж.па инеем покрылась,\n",
        "Х*й стоял, как Дед Мороз!\n",
        "Источник teksty-pesenok.ru\n",
        "\n",
        "Hа Вогрэсовском мосту\n",
        "Церковь обокрали.\n",
        "В ж*пу вые.ли п*па\n",
        "И в колокол наср*ли!\n",
        "\n",
        "Я сосала давеча\n",
        "У Сергея Савича.\n",
        "Он на вид холёненький,\n",
        "А на вкус солёненький!\n",
        "\n",
        "О-па, о-па, зелёная ограда,\n",
        "Девки вые.ли п*па — так ему и надо!\n",
        "\n",
        "Помидоры, помидоры,\n",
        "Помидоры, овощи.\n",
        "П*зда едет на кобыле,\n",
        "Х*й на скорой помощи!\n",
        "\n",
        "Полюбила х*йебина\n",
        "И повесила портрет.\n",
        "А на утро поглядела:\n",
        "Х*й висит, а бина нет!\n",
        "\n",
        "Мы с приятелем вдвоём\n",
        "Работали на дизеле.\n",
        "Он лопух, и я лопух:\n",
        "У нас теплушку сп*здили!\n",
        "\n",
        "Полюбила тракториста\n",
        "И, как водится, дала.\n",
        "Три недели с*ськи мыла\n",
        "И соляркою с.ала!\n",
        "\n",
        "О-па, о-па, зелёная ограда,\n",
        "Девки вые.ли п*па — так ему и надо!\n",
        "\n",
        "Мне подруга дорогая п*рдаёт по рации:\n",
        "\"Я без \"Комбинации\", как без менструации!\"'''\n",
        "\n",
        "\n",
        "songs = [song_1, song_2, song_3, song_4, song_5, song_6, song_7, song_8, song_9, song_10]"
      ],
      "metadata": {
        "id": "unzJsmM4Aoke"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame(songs, columns=['text'])\n",
        "dataset"
      ],
      "metadata": {
        "id": "yRgWgcycDtLA",
        "outputId": "5fbb861b-5cf6-4146-f056-f0d839fc357c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  С матершиной мы родились,\\nC матершиной мы жив...\n",
              "1  Был хорошо, было так легко,\\nНо на шею бросили...\n",
              "2  \\nЯ копаюсь на помойке, как червяк.\\nС детства...\n",
              "3  Бери свою метлу, полетели на шабаш,\\nВозьми с ...\n",
              "4  Солнце осветило горизонт\\nУтро оборвало мой сл...\n",
              "5  Эй, подруга, выходи скорей во двор.\\nЯ специал...\n",
              "6  Скорый поезд к дому мчится.\\nПолечу домой как ...\n",
              "7  Сигарета мелькает во тьме\\nВетер пепел в лицо ...\n",
              "8  Я друзья решил женится правда вот те крест\\nУ ...\n",
              "9  Из-за леса выезжает\\nКонная милиция.\\nСтановис..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8dfb8d48-3b9e-4cd8-82be-e1ca430b374c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>С матершиной мы родились,\\nC матершиной мы жив...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Был хорошо, было так легко,\\nНо на шею бросили...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nЯ копаюсь на помойке, как червяк.\\nС детства...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Бери свою метлу, полетели на шабаш,\\nВозьми с ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Солнце осветило горизонт\\nУтро оборвало мой сл...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Эй, подруга, выходи скорей во двор.\\nЯ специал...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Скорый поезд к дому мчится.\\nПолечу домой как ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Сигарета мелькает во тьме\\nВетер пепел в лицо ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Я друзья решил женится правда вот те крест\\nУ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Из-за леса выезжает\\nКонная милиция.\\nСтановис...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dfb8d48-3b9e-4cd8-82be-e1ca430b374c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8dfb8d48-3b9e-4cd8-82be-e1ca430b374c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8dfb8d48-3b9e-4cd8-82be-e1ca430b374c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz9x-7dWihxx"
      },
      "source": [
        "## 250 Songs\n",
        "\n",
        "Now we've seen a model trained on just a small sample of songs, and how this often leads to repetition as you get further along in trying to generate new text. Let's switch to using the 250 songs instead, and see if our output improves. This will actually be nearly 10K lines of lyrics, which should be sufficient.\n",
        "\n",
        "Note that we won't use the full dataset here as it will take up quite a bit of RAM and processing time, but you're welcome to try doing so on your own later. If interested, you'll likely want to use only some of the more common words for the Tokenizer, which will help shrink processing time and memory needed (or else you'd have an output array hundreds of thousands of words long)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UqmzsyjEA3Sy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWbMN_19jfRT"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LRmPPJegovBe"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kIGedF3XjHj4",
        "outputId": "936ebca4-c649-4d9c-a29f-c7d5964dec90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "# Read the dataset from csv - this time with 250 songs\n",
        "#dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:250]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus, num_words=2000)\n",
        "total_words = tokenizer.num_words\n",
        "\n",
        "# There should be a lot more words now\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quoDmw_FkNBA"
      },
      "source": [
        "### Create Sequences and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kkLAf3HmkPSo"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECbqT-blMk-"
      },
      "source": [
        "### Train a (Better) Text Generation Model\n",
        "\n",
        "With more data, we'll cut off after 100 epochs to avoid keeping you here all day. You'll also want to change your runtime type to GPU if you haven't already (you'll need to re-run the above cells if you change runtimes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7nHOp6uWlP_P",
        "outputId": "0c80dcf9-3b9d-4bb4-8025-01555cabc71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "54/54 [==============================] - 6s 11ms/step - loss: 7.2538 - accuracy: 0.0176\n",
            "Epoch 2/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 6.3624 - accuracy: 0.0265\n",
            "Epoch 3/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 6.2261 - accuracy: 0.0276\n",
            "Epoch 4/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 6.1134 - accuracy: 0.0282\n",
            "Epoch 5/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 6.0071 - accuracy: 0.0312\n",
            "Epoch 6/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.9220 - accuracy: 0.0323\n",
            "Epoch 7/150\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 5.8305 - accuracy: 0.0388\n",
            "Epoch 8/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.7528 - accuracy: 0.0388\n",
            "Epoch 9/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.6842 - accuracy: 0.0370\n",
            "Epoch 10/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.6352 - accuracy: 0.0423\n",
            "Epoch 11/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.5725 - accuracy: 0.0388\n",
            "Epoch 12/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 5.4944 - accuracy: 0.0382\n",
            "Epoch 13/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.4373 - accuracy: 0.0406\n",
            "Epoch 14/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.3850 - accuracy: 0.0476\n",
            "Epoch 15/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 5.3301 - accuracy: 0.0435\n",
            "Epoch 16/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 5.2801 - accuracy: 0.0500\n",
            "Epoch 17/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 5.2319 - accuracy: 0.0523\n",
            "Epoch 18/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 5.1800 - accuracy: 0.0500\n",
            "Epoch 19/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.1295 - accuracy: 0.0541\n",
            "Epoch 20/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.0737 - accuracy: 0.0517\n",
            "Epoch 21/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.0379 - accuracy: 0.0582\n",
            "Epoch 22/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 5.0114 - accuracy: 0.0623\n",
            "Epoch 23/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.9507 - accuracy: 0.0635\n",
            "Epoch 24/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.9108 - accuracy: 0.0658\n",
            "Epoch 25/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.8697 - accuracy: 0.0700\n",
            "Epoch 26/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.8311 - accuracy: 0.0711\n",
            "Epoch 27/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.7821 - accuracy: 0.0741\n",
            "Epoch 28/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.7476 - accuracy: 0.0770\n",
            "Epoch 29/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.7174 - accuracy: 0.0752\n",
            "Epoch 30/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.6792 - accuracy: 0.0788\n",
            "Epoch 31/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.6348 - accuracy: 0.0752\n",
            "Epoch 32/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.6023 - accuracy: 0.0835\n",
            "Epoch 33/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.5824 - accuracy: 0.0864\n",
            "Epoch 34/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.5467 - accuracy: 0.0911\n",
            "Epoch 35/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.4878 - accuracy: 0.0976\n",
            "Epoch 36/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.4541 - accuracy: 0.0999\n",
            "Epoch 37/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.4156 - accuracy: 0.1058\n",
            "Epoch 38/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.3852 - accuracy: 0.1029\n",
            "Epoch 39/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.3593 - accuracy: 0.1035\n",
            "Epoch 40/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.3320 - accuracy: 0.1152\n",
            "Epoch 41/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.2986 - accuracy: 0.1199\n",
            "Epoch 42/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.2541 - accuracy: 0.1217\n",
            "Epoch 43/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.2194 - accuracy: 0.1235\n",
            "Epoch 44/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.1845 - accuracy: 0.1299\n",
            "Epoch 45/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.1513 - accuracy: 0.1276\n",
            "Epoch 46/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 4.1039 - accuracy: 0.1370\n",
            "Epoch 47/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.0839 - accuracy: 0.1346\n",
            "Epoch 48/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 4.0433 - accuracy: 0.1370\n",
            "Epoch 49/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.9977 - accuracy: 0.1358\n",
            "Epoch 50/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.9550 - accuracy: 0.1434\n",
            "Epoch 51/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.9237 - accuracy: 0.1470\n",
            "Epoch 52/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.8869 - accuracy: 0.1481\n",
            "Epoch 53/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.8644 - accuracy: 0.1487\n",
            "Epoch 54/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.8564 - accuracy: 0.1493\n",
            "Epoch 55/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.8167 - accuracy: 0.1481\n",
            "Epoch 56/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.7715 - accuracy: 0.1634\n",
            "Epoch 57/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.7438 - accuracy: 0.1628\n",
            "Epoch 58/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.7281 - accuracy: 0.1617\n",
            "Epoch 59/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.7269 - accuracy: 0.1529\n",
            "Epoch 60/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.6969 - accuracy: 0.1611\n",
            "Epoch 61/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.6350 - accuracy: 0.1723\n",
            "Epoch 62/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.6085 - accuracy: 0.1687\n",
            "Epoch 63/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.5632 - accuracy: 0.1764\n",
            "Epoch 64/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.5396 - accuracy: 0.1758\n",
            "Epoch 65/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.5111 - accuracy: 0.1858\n",
            "Epoch 66/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.4717 - accuracy: 0.1899\n",
            "Epoch 67/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.4344 - accuracy: 0.1922\n",
            "Epoch 68/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.4147 - accuracy: 0.1934\n",
            "Epoch 69/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.3986 - accuracy: 0.1999\n",
            "Epoch 70/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.3938 - accuracy: 0.2034\n",
            "Epoch 71/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.3788 - accuracy: 0.1993\n",
            "Epoch 72/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.3473 - accuracy: 0.2028\n",
            "Epoch 73/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.3104 - accuracy: 0.2140\n",
            "Epoch 74/150\n",
            "54/54 [==============================] - 1s 14ms/step - loss: 3.2944 - accuracy: 0.2263\n",
            "Epoch 75/150\n",
            "54/54 [==============================] - 1s 14ms/step - loss: 3.2493 - accuracy: 0.2281\n",
            "Epoch 76/150\n",
            "54/54 [==============================] - 1s 14ms/step - loss: 3.2302 - accuracy: 0.2269\n",
            "Epoch 77/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.1939 - accuracy: 0.2287\n",
            "Epoch 78/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.1949 - accuracy: 0.2340\n",
            "Epoch 79/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.1735 - accuracy: 0.2434\n",
            "Epoch 80/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 3.1519 - accuracy: 0.2434\n",
            "Epoch 81/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.0879 - accuracy: 0.2516\n",
            "Epoch 82/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.0562 - accuracy: 0.2628\n",
            "Epoch 83/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 3.0221 - accuracy: 0.2745\n",
            "Epoch 84/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.9940 - accuracy: 0.2775\n",
            "Epoch 85/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.9583 - accuracy: 0.2916\n",
            "Epoch 86/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.9469 - accuracy: 0.2934\n",
            "Epoch 87/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.9566 - accuracy: 0.2875\n",
            "Epoch 88/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.9018 - accuracy: 0.2981\n",
            "Epoch 89/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.8680 - accuracy: 0.3139\n",
            "Epoch 90/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.8561 - accuracy: 0.3116\n",
            "Epoch 91/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.8376 - accuracy: 0.3169\n",
            "Epoch 92/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.8504 - accuracy: 0.2992\n",
            "Epoch 93/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.8146 - accuracy: 0.3122\n",
            "Epoch 94/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.7982 - accuracy: 0.3192\n",
            "Epoch 95/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.7589 - accuracy: 0.3245\n",
            "Epoch 96/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.7386 - accuracy: 0.3298\n",
            "Epoch 97/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.7181 - accuracy: 0.3345\n",
            "Epoch 98/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.6800 - accuracy: 0.3480\n",
            "Epoch 99/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.6815 - accuracy: 0.3380\n",
            "Epoch 100/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.6418 - accuracy: 0.3504\n",
            "Epoch 101/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.6035 - accuracy: 0.3545\n",
            "Epoch 102/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.5845 - accuracy: 0.3698\n",
            "Epoch 103/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.5577 - accuracy: 0.3686\n",
            "Epoch 104/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.5686 - accuracy: 0.3692\n",
            "Epoch 105/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.5375 - accuracy: 0.3792\n",
            "Epoch 106/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.5121 - accuracy: 0.3774\n",
            "Epoch 107/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.4739 - accuracy: 0.3921\n",
            "Epoch 108/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.4323 - accuracy: 0.4092\n",
            "Epoch 109/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.3966 - accuracy: 0.4098\n",
            "Epoch 110/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.4171 - accuracy: 0.4062\n",
            "Epoch 111/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.3997 - accuracy: 0.4103\n",
            "Epoch 112/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.4039 - accuracy: 0.4062\n",
            "Epoch 113/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.3676 - accuracy: 0.4215\n",
            "Epoch 114/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.3627 - accuracy: 0.4109\n",
            "Epoch 115/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.3358 - accuracy: 0.4268\n",
            "Epoch 116/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.3149 - accuracy: 0.4439\n",
            "Epoch 117/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.2762 - accuracy: 0.4439\n",
            "Epoch 118/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.2481 - accuracy: 0.4627\n",
            "Epoch 119/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.2641 - accuracy: 0.4468\n",
            "Epoch 120/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.2351 - accuracy: 0.4556\n",
            "Epoch 121/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.2075 - accuracy: 0.4744\n",
            "Epoch 122/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 2.1857 - accuracy: 0.4715\n",
            "Epoch 123/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.1736 - accuracy: 0.4680\n",
            "Epoch 124/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.1654 - accuracy: 0.4797\n",
            "Epoch 125/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.1616 - accuracy: 0.4691\n",
            "Epoch 126/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.1182 - accuracy: 0.4938\n",
            "Epoch 127/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.1259 - accuracy: 0.4815\n",
            "Epoch 128/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.2139 - accuracy: 0.4821\n",
            "Epoch 129/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.1918 - accuracy: 0.4827\n",
            "Epoch 130/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.1067 - accuracy: 0.4962\n",
            "Epoch 131/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.0728 - accuracy: 0.4974\n",
            "Epoch 132/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.0291 - accuracy: 0.5215\n",
            "Epoch 133/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 2.0248 - accuracy: 0.5068\n",
            "Epoch 134/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.9891 - accuracy: 0.5185\n",
            "Epoch 135/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.9685 - accuracy: 0.5238\n",
            "Epoch 136/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.9447 - accuracy: 0.5232\n",
            "Epoch 137/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.9369 - accuracy: 0.5320\n",
            "Epoch 138/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.9210 - accuracy: 0.5273\n",
            "Epoch 139/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.8921 - accuracy: 0.5326\n",
            "Epoch 140/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.8797 - accuracy: 0.5356\n",
            "Epoch 141/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.8633 - accuracy: 0.5409\n",
            "Epoch 142/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.8324 - accuracy: 0.5567\n",
            "Epoch 143/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.8017 - accuracy: 0.5697\n",
            "Epoch 144/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.7934 - accuracy: 0.5626\n",
            "Epoch 145/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.8262 - accuracy: 0.5585\n",
            "Epoch 146/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.8147 - accuracy: 0.5497\n",
            "Epoch 147/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.7987 - accuracy: 0.5655\n",
            "Epoch 148/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.7390 - accuracy: 0.5797\n",
            "Epoch 149/150\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 1.7125 - accuracy: 0.5891\n",
            "Epoch 150/150\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.7694 - accuracy: 0.5773\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=150, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgvIz20nlQcq"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOqmmarvlSLh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISLZZGlQlSxh"
      },
      "source": [
        "### Generate better lyrics!\n",
        "\n",
        "This time around, we should be able to get a more interesting output with less repetition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P96oVMk3lU7y",
        "outputId": "d7d91982-b541-4d83-9dda-8c403d65d017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Если б я был султан им был всё устал бльё правда через туман то правда матом крест туман одет надо те интерес те те матом через туман туман прекрасно пзда живу прекрасно крест решил те крест те через твоего туман туман мать те крест те матом туман туман туман туман туман то ядрена те медь те крест те матом жалейте твоего твоего твоего прекрасно живу мать крест те крест те матом мне матершиной туман надо те интерес те своей интерес те матом туман туман сном бычок во 100 100 крест те крест те те через мелькает вред плевать через крест туман туман без те крест\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"Если б я был султан\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgJKV8_oRU9"
      },
      "source": [
        "### Varying the Possible Outputs\n",
        "\n",
        "In running the above, you may notice that the same seed text will generate similar outputs. This is because the code is currently always choosing the top predicted class as the next word. What if you wanted more variance in the output? \n",
        "\n",
        "Switching from `model.predict_classes` to `model.predict_proba` will get us all of the class probabilities. We can combine this with `np.random.choice` to select a given predicted output based on a probability, thereby giving a bit more randomness to our outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lZe9gaJeoGVP",
        "outputId": "6319eb14-2277-4601-dc4c-de6c276f7eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "# Test the method with just the first word after the seed text\n",
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "predicted_probs = model.predict(token_list)[0]\n",
        "predicted = np.random.choice([x for x in range(len(predicted_probs))], \n",
        "                             p=predicted_probs)\n",
        "# Running this cell multiple times should get you some variance in output\n",
        "print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ee7WKgRGrJy1",
        "outputId": "ec3d9449-3b1f-4c23-8298-a4657e6c7612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Есди б я был султан возьму они про в натуре затяну бомж не отцом стучит злой не 10 дым ведь крест никому щекою очень налей люцифер крест возьмись килограмм  афер пылающего налей люцифер грамм вскоре ж поройбери        детям птица рассвет туман прекрасно учти хвосты матом туман без крест свою билет очень 100 говорит где отцом афер с устал освещает путь земли твоегоа к  люцифер афер аркан мне tekstypesenokru жизнь гитару наперекосяк тебя я колхоз видно на похожий сочинял мы дед в видно меня я стояла как ребята я черный люцифер матери дождь им с еет крест\n"
          ]
        }
      ],
      "source": [
        "# Use this process for the full output generation\n",
        "seed_text = \"Есди б я был султан\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\n",
        "                               p=predicted_probs)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c04_nlp_optimizing_the_text_generation_model.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}